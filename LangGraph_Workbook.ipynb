{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astar Consulting - LangGraph Workshop\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./images/Astar Consulting.png\" alt=\"levels of automation\" width='80%'>\n",
    "</div>\n",
    "\n",
    "Welcome to the LangGraph Workshop! This workshop is designed to help you understand the basic concepts of building LLM applications with a focus on the LangGraph library.\n",
    "\n",
    "**Astar Consulting:**\n",
    "- [Website](https://www.astarconsulting.no/)\n",
    "- [LinkedIn](https://www.linkedin.com/company/astarconsulting)\n",
    "\n",
    "- [Erik Nymo Bohne](https://www.linkedin.com/in/erik-nymo-bohne-25868321a/)\n",
    "- [Mikael Steenbuch](https://www.linkedin.com/in/mikael-steenbuch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "- [Langchain](https://python.langchain.com/v0.2/docs/introduction/)ü¶ú\n",
    "- [Langgraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)üï∏Ô∏è\n",
    "- [Langsmith](https://docs.smith.langchain.com/)üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levels of automation - Motivation\n",
    "The rapid development in large language models and frameworks has made it possible to automate a wide range of tasks. The levels of automation can be seen as a scale of how much human intervention is needed to complete a task. The scale ranges from 1 to 6, where 1 is no automation and 6 is full automation. With LangGraph we are designing systems to achieve level 4 and 5, and even reacing level 6 in some cases.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./images/LevelsOfAutomation.png\" alt=\"levels of automation\" width='80%'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langgraph langsmith langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your_key\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph Tutorial\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-keUrFilXq-v8soqanKzZWKjmeTRIcfFNuNNRQOney1T3BlbkFJfXcKvwluG426stbMGLL0ADmobqOWS2atbDth4VqjIA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The State of the graph\n",
    "The first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the schema of the graph as well as `reducer functions` which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified reducer function.\n",
    "\n",
    "Source: [Langchain AI - State of the graph](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Nodes for the graph\n",
    "In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a \"config\", containing optional configurable parameters (such as a thread_id).\n",
    "\n",
    "Source: [Langchain AI - Nodes](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "def chatbot(state: GraphState):\n",
    "    \"\"\"\n",
    "    Simple bot that invokes the list of previous messages\n",
    "    and returns the result which will be added to the list of messages.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = (\"human\", \"Hva er hovedstaden i Norge?\")\n",
    "chatbot({\"messages\": [message]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Edges - The workflow of the graph\n",
    "Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\n",
    "\n",
    "- **Normal Edges**: Go directly from one node to the next.\n",
    "- **Conditional Edges**: Call a function to determine which node(s) to go to next.\n",
    "- **Entry Point**: Which node to call first when user input arrives.\n",
    "- **Conditional Entry Point**: Call a function to determine which node(s) to call first when user input arrives.\n",
    "\n",
    "A node can have **MULTIPLE** outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.\n",
    "\n",
    "Source: [Langchain AI - Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Adding nodes to the workflow\n",
    "workflow.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Defining edges between nodes\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "workflow.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke/stream the graph\n",
    "LangGraph is built with first class support for streaming. There are several different ways to stream back results from the graph.\n",
    "\n",
    "Source: [Langchain AI - Streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    print(\"User: \" + user_input)\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lets add tools to the graph\n",
    "\n",
    "It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools.\n",
    "\n",
    "Source: [Langchain AI - Tools](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#tool-calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: GraphState):\n",
    "    \"\"\"\n",
    "    Simple bot that invokes the list of previous messages\n",
    "    and returns the result which will be added to the list of messages.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Adding nodes to the workflow\n",
    "workflow.add_node(\"chatbot\", chatbot)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Defining edges between nodes\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "workflow.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# Defining conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    print(\"User: \", user_input)\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            if isinstance(value[\"messages\"][-1], BaseMessage):\n",
    "                print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: CREATE YOUR OWN TOOL\n",
    "\n",
    "#### Suggestions:\n",
    "\n",
    "**Easy:**\n",
    "1. Create a tool that performs a simple mathematical operation (e.g. addition, matrix multiplication, etc.)\n",
    "2. Create a tool that performs a simple string operation (e.g. string concatenation, string split, etc.)\n",
    "\n",
    "**Medium:**\n",
    "1. Create a tool that performs a simple data analysis operation (e.g. Pandas.DataFrame.describe())\n",
    "2. Create a tool that reads/writes a file.\n",
    "\n",
    "**Hard:**\n",
    "1. Create a tool that writes and executes python code.\n",
    "2. Create a tool that gathers information from a website or via an API.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example tool:**\n",
    "```python\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```\n",
    "\n",
    "#### Documentation:\n",
    "- [Langchain AI - Tools](https://python.langchain.com/v0.2/docs/how_to/tool_calling/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "@tool\n",
    "#-----------TASK 1: Write your own tool--------------\n",
    "def my_tool():\n",
    "    \"\"\"Your tool description here\"\"\"\n",
    "    pass\n",
    "#-----------------------------------------------------\n",
    "\n",
    "tools = [my_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_with_your_tool = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot_with_your_tool(state: GraphState):\n",
    "    \"\"\"\n",
    "    Simple bot that invokes the list of previous messages\n",
    "    and returns the result which will be added to the list of messages.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm_with_your_tool.invoke(state[\"messages\"])]}\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"chatbot_with_your_tool\", chatbot_with_your_tool)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "workflow.add_edge(START, \"chatbot_with_your_tool\")\n",
    "workflow.add_edge(\"tools\", \"chatbot_with_your_tool\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"chatbot_with_your_tool\",\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"User: \")\n",
    "for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "    for value in event.values():\n",
    "        if isinstance(value[\"messages\"][-1], BaseMessage):\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulationsüéâ You have now created your first tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output\n",
    "\n",
    "It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state.\n",
    "\n",
    "Source: [Langchain AI - Structured output](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#structured-output)\n",
    "\n",
    "A structured output for weather might look like this:\n",
    "```json\n",
    "{\n",
    "  \"temperature\": 25,\n",
    "  \"humidity\": 0.5,\n",
    "  \"wind_speed\": 10,\n",
    "  \"wind_direction\": \"N\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ResumeSummary(BaseModel):\n",
    "    name: str = Field(..., title=\"Name if the applicant\")\n",
    "    experience: List[str] = Field(..., title=\"List of experiences\")\n",
    "    skills: List[str] = Field(..., title=\"List of skills\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the LangChain `.with_structured_output()` method, we can specify the output format of the tool. This will allow us to generate structured output from the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    {application}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt | llm.with_structured_output(ResumeSummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load some applications and test the structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "applications = []\n",
    "folder_path = os.path.join(\"data\", \"applications\")\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            applications.append(file.read())\n",
    "\n",
    "for application in applications:\n",
    "    print(application)\n",
    "    print(chain.invoke(application).json(indent=2))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: **Create a Structured Output from Unstructured Emails**\n",
    "\n",
    "You are provided with 5 `.txt` files, each containing an email about an office supply order. The emails may include:\n",
    "\n",
    "- Product names, quantities\n",
    "- Delivery dates, addresses\n",
    "- Customer details (names, contacts)\n",
    "\n",
    "Your task is to create a structured object that captures the relevant data (e.g., products, quantities, delivery info) while handling potential noise and inconsistencies. The structure should be designed for future analysis and flexibility to deal with incomplete or messy information.\n",
    "\n",
    "### Example Output:\n",
    "```json\n",
    "{\n",
    "    \"customer\": {\n",
    "        \"name\": \"John Doe\", \n",
    "        \"company\": \"ACME Corp\"\n",
    "    },\n",
    "    \"order_items\": [\n",
    "        {\n",
    "            \"product\": \"Pens\",\n",
    "            \"quantity\": 10\n",
    "        }\n",
    "    ],\n",
    "    \"delivery\": {\n",
    "        \"address\": \"123 Main St\",\n",
    "        \"date\": \"2023-09-12\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Documentation:\n",
    "\n",
    "- [Langchain AI - Structured Output](https://python.langchain.com/v0.2/docs/how_to/structured_output/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = []\n",
    "folder_path = os.path.join(\"data\", \"orders\")\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            orders.append(file.read())\n",
    "\n",
    "# See how one of the orders look\n",
    "print(orders[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You job now is to create the Pydantic object that will be used to structure the information in the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------TASK 2: Write your pydantic object --------------\n",
    "class Order(BaseModel):\n",
    "    pass\n",
    "#------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    {order}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt | llm.with_structured_output(Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders[0]\n",
    "print(chain.invoke(orders[0]).json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Reflection\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./images/Reflection.png\" alt=\"basic agent reflection\" width='50%'>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm_gpt_3_5 = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    essay: str\n",
    "    feedback: Annotated[list, add_messages]\n",
    "\n",
    "def essay_writer(state: GraphState):\n",
    "    \"\"\"Node that generate a 3 paragraph essay\"\"\"\n",
    "    print(\"\\n---ESSAY WRITER---\")\n",
    "    essay = state[\"essay\"] if \"essay\" in state else \"No essay yet\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Write a 3 paragraph essay based on the following feedback:\n",
    "\n",
    "        Essay: {essay}\n",
    "\n",
    "        Update the essay based on the feedback:\n",
    "\n",
    "        Feedback: {feedback}\n",
    "        \"\"\"\n",
    "    )\n",
    "    generate = prompt | llm_gpt_3_5 | StrOutputParser()\n",
    "    essay = generate.invoke({\"essay\": essay, \"feedback\": state[\"feedback\"]})\n",
    "    print(\"\\nEssay: \", essay)\n",
    "    return {\"essay\": [essay]}\n",
    "\n",
    "\n",
    "def essay_grader(state: GraphState):\n",
    "    \"\"\"Node that grades an essay\"\"\"\n",
    "    print(\"\\n---ESSAY GRADER---\")\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a teacher grading an essay. Provide clear and consise feedback on how to improve the essay:\n",
    "\n",
    "        Essay: {essay}\n",
    "        \"\"\"\n",
    "    )\n",
    "    generate = prompt | llm\n",
    "    feedback = generate.invoke({\"essay\": state[\"essay\"] if \"essay\" in state else \"No essay yet\"})\n",
    "    print(\"\\nFeedback: \", feedback)\n",
    "    return {\"feedback\": [feedback]}\n",
    "\n",
    "\n",
    "def should_continue(state: GraphState):\n",
    "    \"\"\"Node that checks if the user wants to continue\"\"\"\n",
    "    print(\"\\n---SHOULD CONTINUE---\")\n",
    "    if len(state[\"feedback\"]) > 3:\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        return \"essay_grader\"\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"essay_writer\", essay_writer)\n",
    "workflow.add_node(\"essay_grader\", essay_grader)\n",
    "\n",
    "workflow.add_edge(START, \"essay_writer\")\n",
    "workflow.add_edge(\"essay_grader\", \"essay_writer\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"essay_writer\",\n",
    "    should_continue\n",
    ")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Skriv en saktekst om Norges Teknisk-Naturvitenskapelige Universitet i Trondheim.\"\n",
    "\n",
    "essay = graph.invoke({\"feedback\": [request]})\n",
    "print(\"Essay: \", essay[\"essay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: **Create a code-generating agent with self reflection**\n",
    "\n",
    "### Motivation\n",
    "Code generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution.\n",
    "\n",
    "### Task\n",
    "Create an agent that generates code based on what the user wants. Take a look at this flow diagram:\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"./images/Code-Generation.png\" alt=\"basic agent reflection\" width='50%'>\n",
    "</div>\n",
    "\n",
    "You are free to choose the complexity of the task with these suggestions:\n",
    "1. Code execution: Create a tool that executes the code generated by the agent.\n",
    "2. Documentation: Create a tool that retrieves the documentation of a given library.\n",
    "3. Add support for multiple programming languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------TASK 3.1: Write the GraphState and potentially any pydantic objects--------------\n",
    "class GeneratedCode(BaseModel):\n",
    "    pass\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------TASK 3.2: Define any tools that you may use--------------\n",
    "@tool\n",
    "def my_tool():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------TASK 3.3: Write the nodes--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "#-----------TASK 3.4: Define the workflow--------------\n",
    "\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now its time to see if the agent will workü§©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = input(\"What do you want to generate? \")\n",
    "generated_code = graph.invoke({\"request\": request})\n",
    "print(\"Generated Code: \", generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the end of the workshop. We hope you enjoyed it.üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
